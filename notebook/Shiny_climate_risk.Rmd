---
title: "PARM toolkit - Weather risks assessment"
subtitle: "Input data sheme"
author: 'A. Esquivel  - H. Achicanoy'
date: "2/22/2021"
output: html_document
runtime: shiny
---

#### GitHub Repository --- (<https://github.com/haachicanoy/climate-risk-profiles>)


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

suppressMessages(library(pacman))
suppressMessages(pacman::p_load(dplyr, tidyverse, DT,  sf, leaflet,  
                                raster, ncdf4, rgdal, shiny))
# library(shiny); library(shinyFiles)

# source('D:/OneDrive - CGIAR/Desktop/PARM_toolkit/climate-risk-profiles/main_script.R')


```
<br>

The developed model provides a country/regional overview of the potential weather risks over historical periods, estimating the likely impact and the regions where it is going to take place.


The proposed method consists of a time series analysis of climate data. Temperature, precipitation, solar radiation, and soil variables are using to calculate agro-climatic indices that quantify the impact of historical changes over the main value chains analyzed in the study region. The analysis is carried out at 5 km resolution.

Following, a detailed description about the inputs, methodology, and results is presented.

<br>


### Data description

<br>

#### Variables and climate data sets: 

Historical climate information is the base to estimate the likely impact and regions where it is going to take place, the most important are: 

+ Precipitation (mm/day): we use the information from the Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) daily dataset (<https://www.chc.ucsb.edu/data/chirps>), it is a 35+ year rainfall data set. Spanning 50°S-50°N (and all longitudes), the time period covered 1981 to near-present. CHIRPS has 0.05° resolution satellite imagery. For more information, refer to the web site or read Funk, et all, 2015.

+ Temperature (°C/day):  in this case we need to get data of Tmax, Tmin y Tmean. CHIRTS daily dataset (<https://www.chc.ucsb.edu/data/chirtsdaily>) provides the daily maximum and minimum temperatures. CHIRTS is quasi-global (60°S – 70°N, for all longitudes), and their resolution is 0.05°, approx. 5km). This data set is available between 1983 to 2016. 

+ Solar radiation (MJ m^(-2) d^(-1)): This variable was taken from the National Aeronautics and Space Administration (NASA). The dataset contains daily incident solar radiation, Tmax, Tmin, dew point temperature (Tdew), precipitation, wind speed, and relative humidity (RH) data for each 1° × 1° grid (approximately 111 km2 at the equator). This data is available on NASA POWER’s web site (<https://power.larc.nasa.gov>).

Moreover, we also need to incorporate location data like: 

+ Elevation (m): This variable is downloaded using the function getData from the raster package in R. The argument 'alt' stands for altitude (elevation); the data were aggregated from SRTM 90m. GADM is the database of global administrative boundaries used by R (<https://gadm.org>). 


+ Soil:  this data is from the ISRIC SoilGrids database. This is a global 30 arc-sec database derived through interpolation of soil samples (<https://www.isric.org/explore/soilgrids>).
Before using these variables in the analysis, a data standardization process is developed to get the final resolution 0.05°, approx. 5km, and the modelling is done county by county. Additionally, the study period covered a 30-years (1985-2015).




## Modelling 

Next, we are going to explain how you can run the models using R-project. Please keep in mind, that you must have a specific version of R, R studio and install certain packages, so it's suggested that you check the requirements in the annexes before running the methodology.

### input data


<br>

<div align="center"> _**Before you run:**  We advise that you analyze a small region, because if your machine doesn't have good processing power it’s likely that the script won’t run. Also, some process run in parallel, for this reason is important to check how many cores your machine have, always use less that the total amount of cores. However, keep in mind; **more cores use more RAM memory**. _<div>

<br>


#### Parameters.

<div align="left"> 

 <div>




```{r, echo=FALSE}
count_names <- raster::ccodes() %>% as_tibble()  %>%
  mutate(Country = iconv(NAME,from="UTF-8",to="ASCII//TRANSLIT")) %>%
  dplyr::select(Country, ISO3)

# =-------
climate_exp <- tibble(month_1 = 1:12, month = month.abb, panel = c(1:6, 6:1))

p <- ggplot2::ggplot(data = climate_exp, aes(x =forcats::fct_reorder(month.abb, 1:12)  , y = c(1:6, 6:1))) +
  geom_bar(stat = 'identity', position = 'dodge', fill = '#9ecae1') +
  labs(x =  NULL, y = NULL) +
  theme_bw() +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.border=element_blank())

# =-----------------------------------------------------------
adm_lvl =  1 
iso3c <- 'BFA'
shp_data <- raster::getData('GADM', country= iso3c, level=adm_lvl)
shp_data@data[, glue::glue('NAME_{adm_lvl}')] <-
  dplyr::pull(shp_data@data, glue::glue('NAME_{adm_lvl}')) %>%
  stringr::str_replace_all(., ' ', '-') %>%
  stringr::str_remove('[("!\"#$%()*,.:;<=>@[\\]^`{|}~.\']]') %>%
  stringi::stri_trans_general(., "Latin-ASCII")

shp <- shp_data %>% sf::st_as_sf()
cty <- filter(shp, NAME_1 == 'Sud-Ouest')
b <- suppressWarnings(st_centroid(cty$geometry) %>% st_coordinates()  )

# =-----------------------------------------------------------


max_cores <- parallel::detectCores() - 1

# DT::renderDataTable(count_names)

# Funciones para el guardado de los parametros.
results_dir <- 'D:/OneDrive - CGIAR/Desktop/PARM_toolkit/climate-risk-profiles/responses/'

saveData <- function(data) {
    data <- t(data)
    write.csv(x = data, file = 'parameters.csv',
        row.names = FALSE, quote = TRUE)
}

load_data <- function() {
    data <- read_csv(paste0(results_dir, 'parameters.csv'))
    data
}


# Define the fields we want to save from the form
fields <- c("Adm_lvl", "county","seasons", "m_seasons",
            "n_wtts", "Big_cnt", "ncores") #

# Shiny app with 3 fields that the user can submit data for
shiny::shinyApp(
  ui = fluidPage(

      # Sidebar layout with input and output definitions ----
  sidebarLayout(

    # Sidebar panel for inputs ----
    sidebarPanel(

    titlePanel("Parameters."),

    # radioButtons("ty", label = "Type of season",
    # choices = list("Manual" = 1, "Automatic" = 2),
    # selected = 1),
    checkboxInput("Big_cnt", "Big county", FALSE),
     helpText('If this argument is TRUE, it uses a 30%-pixels sample of the entire dataset to compute the indices, and then, it does an interpolation for the whole county’s pixels.'),


     numericInput("ncores", label = "cores", value = 1, min = 1, max = max_cores),


    actionButton("submit", "Submit")
  ),
      mainPanel(
      tabsetPanel(
          tabPanel("Country Names",
                   DT::dataTableOutput('table'),
                   verbatimTextOutput('x5')),
          tabPanel("Season",
                  tabsetPanel(id = "tabset",
          tabPanel("Manual",
          fluidRow(
       column(6,selectInput("Int", label = 'Int', choices = month.abb, selected = 'Jan'),
       ),
    column(6,selectInput("End", label = 'End', choices = month.abb, selected = 'Dec'),
    ),
    ),
    plotOutput("plot2"), ),
        tabPanel("Automatic",
     fluidRow(
       column(5,selectInput("m_seasons", label = 'm_seasons',
      choices = list(1, 2), selected = 1),  ),
    column(5, numericInput("n_wtts", label = "n_wtts", value = 100, min = 10, max = 365), ),  ),  ) )

    ), # verbatimTextOutput("summary")
          tabPanel("Spatial Region",
   fluidRow(
    column(5,sliderInput("Adm_lvl", "Administrative level",
                0, 2, 1)),
    column(5,textInput("county", "County", "") )  ),
    leafletOutput("map")) 
        ),  ),  ), ),

  server = function(input, output, session) {

    output$table = DT::renderDataTable(count_names, server = FALSE)

    output$x5 = renderPrint({
    cat('Rows on the current page:\n\n')
    cat(input$table_rows_current, sep = ', ')
    cat('\n\nAll rows:\n\n')
    cat(input$table_rows_all, sep = ', ')
    cat('\n\nSelected rows:\n\n')
    cat(input$table_rows_selected, sep = ', ')})



   formData <- reactive({
      data <- sapply(fields, function(x) input[[x]])
      s = input$table_rows_selected
      data <- c(data, data.frame(count_names[s, ]))
      data
    })


   output$plot2 <- renderPlot({
    f <-  p + geom_rect(xmin = input$Int, xmax = input$End , ymin=0, ymax = 6.5, fill = '#ffb3b3', alpha = 0.05)
    f })

    # When the Submit button is clicked, save the form data
    observeEvent(input$submit, {
      saveData(formData())
    })

    # Show the previous responses
    # (update with current response when Submit is clicked)
    output$responses <- DT::renderDataTable({
      input$submit
      loadData()
    })    
    
    
    output$map <- leaflet::renderLeaflet(     
      
      leaflet() %>% addTiles() %>%  setView(-3.23,10.47, zoom = 7) %>%
      leaflet::addPolygons(data = shp, color = "gray",
                       weight = 1, smoothFactor = 0.5,
                       opacity = 1.0, fillOpacity = 0.5)  %>%
    leaflet::addPolygons(data = cty, color = "red", 
                       highlightOptions = highlightOptions(color = "white",weight = 2,bringToFront = TRUE))
    
  )
      
      
      


    
    
    }
)
```
<br>



<div align="left"> 


### Steps to run the scripts (overview):

1.	Historical data extraction: using free-climate databases, a spatial-temporal database is constructed for the specific country/region of interest. This database consists in a table format that storage the time series per location/pixel of (tmax, tmin, prec, srad).

2.	Data processing is applied to guarantee good quality of the data.

3.	Time series-indices construction: to quantify the weather risks that are present in the region, the method estimates indices on three categories: heat, drought, and moisture/flood risks. These calculations are developed by seasons, according to the knowledge of the value chains of interest.

4.	Finally, a quantification of the trends and changes over spatial locations is done, and combined with all the risks by the assessment of the frequency and strength of the indices.


 <div>


## References. 

+ Funk, C., Peterson, P., Landsfeld, M., Pedreros, D., Verdin, J., Shukla, S., ... & Michaelsen, J. (2015). The climate hazards infrared precipitation with stations—a new environmental record for monitoring extremes. Scientific data, 2(1), 1-21.

+ Funk, C., Peterson, P., Peterson, S., Shukla, S., Davenport, F., Michaelsen, J., ... & Mata, N. (2019). A High-Resolution 1983–2016 T max Climate Data Record Based on Infrared Temperatures and Stations by the Climate Hazard Center. Journal of Climate, 32(17), 5639-5658.

+ Zhang, T., Stackhouse Jr, P. W., Chandler, W. S., Hoell, J. M., WESTBERG, D. J., & Whitlock, C. H. (2007). A global perspective: NASA's prediction of worldwide energy resources (POWER) project.


